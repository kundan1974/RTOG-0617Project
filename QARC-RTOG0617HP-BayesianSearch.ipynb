{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model with Bayesian Searched hyperparameter\n",
    "\n",
    "We will be using this library for hyperparameter search with bayesian optimization method\n",
    "\n",
    "pip install bayesian-optimization\n",
    "\n",
    "        or\n",
    "\n",
    "conda install -c conda-forge bayesian-optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv3D,MaxPool3D,Flatten,Dropout,BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "import gc\n",
    "import matplotlib\n",
    "import nvgpu\n",
    "import multiprocessing\n",
    "import neptune\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvgpu.gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gc.collect()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alive = \"/home/airadiomicslab/OneDrive/RTOGData/ready64_18M_HD/alive\"\n",
    "train_dead  = \"/home/airadiomicslab/OneDrive/RTOGData/ready64_18M_HD/dead/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patient_images(input_path):\n",
    "    np.load(input_path)\n",
    "    return np.array(np.load(input_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alivelist = os.listdir(train_alive)\n",
    "deadlist = os.listdir(train_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list_alive = []\n",
    "path_list_dead = []\n",
    "for i in alivelist:\n",
    "    path_list_alive.append(os.path.join(train_alive,i))\n",
    "for i in deadlist:\n",
    "    path_list_dead.append(os.path.join(train_dead,i))\n",
    "    \n",
    "labels_alive = np.zeros(len(alivelist))\n",
    "labels_alive = list(labels_alive)\n",
    "labels_dead = np.ones(len(deadlist))\n",
    "labels_dead = list(labels_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "images = []\n",
    "\n",
    "slices_alive = [load_patient_images(path_list_alive[s]) for s in range(0,len(path_list_alive))]\n",
    "slices_dead = [load_patient_images(path_list_dead[k]) for k in range(0,len(path_list_dead))]\n",
    "\n",
    "images.append(slices_alive.copy())\n",
    "images[0].extend(slices_dead.copy())\n",
    "images = np.array(images)\n",
    "images = images[0,:,:,:,:]\n",
    "slices_alive.clear()\n",
    "slices_dead.clear()\n",
    "\n",
    "labels.append(labels_alive.copy())\n",
    "labels[0].extend(labels_dead.copy())\n",
    "labels = np.array(labels)\n",
    "labels = labels[0,:]\n",
    "labels_alive.clear()\n",
    "labels_dead.clear()\n",
    "\n",
    "print(f\"Labels Shape for input into CNN: {labels.shape}\")\n",
    "print(f\"Data Shape for input into CNN: {images.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.astype(dtype='float32')\n",
    "images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.astype(dtype='uint8')\n",
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(images, labels, test_size = 0.2)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape for Training Set is: {xtrain.shape} and corrsponding labels shape is: {ytrain.shape} \")\n",
    "print(f\"Shape for Test Set is:     {xtest.shape} and corrsponding labels shape is:  {ytest.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape(len(xtrain),64,64,64,1)\n",
    "xtest = xtest.reshape(len(xtest),64,64,64,1)\n",
    "\n",
    "print(f\"New Shape for Training data required for MODEL is: {xtrain.shape} \")\n",
    "print(f\"New Shape for Test data is: required for MODEL is: {xtest.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain = xtrain.astype(dtype='float32')\n",
    "#xtest = xtest.astype(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ytrain = ytrain.astype(dtype='uint8')\n",
    "#ytest = ytest.astype(dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xtrain[15][:,:,32,0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepathAcc = \"/home/airadiomicslab/OneDrive/BayesianModels/RTOGBayesianBestAccModel.hdf5\"\n",
    "filepathLoss = \"/home/airadiomicslab/OneDrive/BayesianModels/RTOGBayesianBestLossModel.hdf5\"\n",
    "checkpointAcc = [ModelCheckpoint(filepathAcc,monitor='val_accuracy',verbose=1,save_best_only=True,mode='max')]\n",
    "checkpointLoss = [ModelCheckpoint(filepathLoss,monitor='val_loss',verbose=1,save_best_only=True,mode='min')]\n",
    "early_stop = [EarlyStopping(monitor='val_loss',patience=10)]\n",
    "callback_list = [early_stop,checkpointAcc,checkpointLoss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape=(64,64,64,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2SaveModel = \"/home/airadiomicslab/OneDrive/RTOGModels/\"\n",
    "path2SaveError = \"/home/airadiomicslab/OneDrive/RTOGRunErrors/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QARCEsoModel(batch_size,learning_rate,L2_2,L2_3,BNConv1,BNConv2,BNDense1,\n",
    "                 BNDense2,BNDense3):\n",
    "    \n",
    "    model = Sequential() \n",
    "    model.add(Conv3D(filters=16,kernel_size=(5,7,5),strides=(1,1,1),padding='same',\n",
    "                     input_shape=image_shape, activation='relu',name='Conv3D_InputLayer')) \n",
    "    model.add(MaxPool3D(pool_size=(2,3,2),name='MaxPool4InputLayer'))\n",
    "\n",
    "    if L2_2:\n",
    "        L2_2 = 0.01\n",
    "    else:\n",
    "        L2_2 = 0.001\n",
    "        \n",
    "    model.add(Conv3D(filters=32,kernel_size=(3,3,3),strides=(1,1,1),\n",
    "                     padding='same',kernel_regularizer=tf.keras.regularizers.l2(L2_2),activation='relu',\n",
    "                     name='Conv3D_FirstLayer'))\n",
    "    model.add(MaxPool3D(pool_size=(2,3,2),name='MaxPool4FirstLayer'))\n",
    "    if BNConv1:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    if L2_3:\n",
    "        L2_3= 0.01\n",
    "    else:\n",
    "        L2_3 = 0.001\n",
    "\n",
    "    model.add(Conv3D(filters=128,kernel_size=(3,3,3),strides=(1,1,1),\n",
    "                     padding='same',kernel_regularizer=tf.keras.regularizers.l2(L2_3),activation='relu',\n",
    "                     name='Conv3D_SecondLayer'))\n",
    "    model.add(MaxPool3D(pool_size=(3,3,3),name='MaxPool4SecondLayer'))\n",
    "    if BNConv2:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    if BNDense1:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    if BNDense2:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    if BNDense3:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    if learning_rate == 0:\n",
    "        learning_rate = 0.01\n",
    "    elif learning_rate == 1:\n",
    "        learning_rate = 0.01\n",
    "    elif learning_rate == 2:\n",
    "        learning_rate = 0.001\n",
    "    elif learning_rate == 3:\n",
    "        learning_rate = 0.0001\n",
    "    else:\n",
    "        learning_rate = 0.00001\n",
    "        \n",
    "    optimizer = Adam(learning_rate = learning_rate, name='Adam')\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    # INSERT CHECKPOINT FOR SAVING MODEL\n",
    "    model.fit(xtrain,ytrain,batch_size=batch_size,epochs=500,validation_data=(xtest,ytest),\n",
    "              steps_per_epoch=xtrain.shape[0]//batch_size,\n",
    "              callbacks=early_stop,verbose=1,shuffle=True)\n",
    "    _, accuracy = model.evaluate(xtest,ytest,batch_size=8)\n",
    "    return accuracy,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cnn(batch_size,learning_rate,L2_2,L2_3,BNConv1,BNConv2,BNDense1,BNDense2,BNDense3):\n",
    "    global session_num\n",
    "    global last_acc\n",
    "    # define parameters\n",
    "    params = {\n",
    "        'batch_size': int(np.around(batch_size)),\n",
    "        'learning_rate' :  int(np.around(learning_rate)),\n",
    "        'L2_2' : int(np.around(L2_2)),\n",
    "        'L2_3' : int(np.around(L2_3)),\n",
    "        'BNConv1' : int(np.around(BNConv1)),\n",
    "        'BNConv2' : int(np.around(BNConv2)),\n",
    "        'BNDense1' : int(np.around(BNDense1)),\n",
    "        'BNDense2' : int(np.around(BNDense2)),\n",
    "        'BNDense3' : int(np.around(BNDense3))\n",
    "    }\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    session_num +=1\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    #print({params.name: params[h] for h in params})\n",
    "    for value in params:\n",
    "        print(f'Parameter values for this run are {value}: {params[value]}')\n",
    "    try:\n",
    "        # select project\n",
    "        #neptune.set_project('kundan25/EsoCRPredictionBaysian') # NOT SURE TO USE IT OR NOT\n",
    "        #neptune.init('kundan25/EsoCRPredictionBaysian')\n",
    "        with neptune.create_experiment(name='RTOG0617ModelRun3',params=params) as BayesianOptimizer:\n",
    "            valid_acc,model = QARCEsoModel(**params)\n",
    "            last_acc = valid_acc\n",
    "            if (valid_acc > 0.95):\n",
    "                save_model(model,filepath=path2SaveModel+'bestmodel95' + run_name +'.hdf5')\n",
    "                BayesianOptimizer.append_tag('BestAcc95')\n",
    "            elif (valid_acc > 0.90):\n",
    "                save_model(model,filepath=path2SaveModel+'bestmodel90' + run_name +'.hdf5')\n",
    "                BayesianOptimizer.append_tag('BestAcc90')\n",
    "            elif (valid_acc > 0.85):\n",
    "                save_model(model,filepath=path2SaveModel+'bestmodel85' + run_name +'.hdf5')\n",
    "                BayesianOptimizer.append_tag('BestAcc85')\n",
    "            elif (valid_acc > 0.80):\n",
    "                save_model(model,filepath=path2SaveModel+'bestmodel80' + run_name +'.hdf5')\n",
    "                BayesianOptimizer.append_tag('BestAcc80')\n",
    "            for i in model.history.history['accuracy']:\n",
    "                BayesianOptimizer.log_metric('Training Accuracy', i)\n",
    "                BayesianOptimizer.log_metric('Max_Training_Accuracy', max(model.history.history['accuracy']))\n",
    "            for i in model.history.history['val_accuracy']:\n",
    "                BayesianOptimizer.log_metric('Validation_Accuracy', i)\n",
    "                BayesianOptimizer.log_metric('Max Val_Accuracy', max(model.history.history['val_accuracy']))\n",
    "            for i in model.history.history['loss']:\n",
    "                BayesianOptimizer.log_metric('Training Loss', i)\n",
    "                BayesianOptimizer.log_metric('Min Training_Loss', min(model.history.history['loss']))\n",
    "            for i in model.history.history['val_loss']:\n",
    "                BayesianOptimizer.log_metric('Validation Loss', i)\n",
    "                BayesianOptimizer.log_metric('Min Val_Loss', min(model.history.history['val_loss']))\n",
    "            tf.keras.backend.clear_session()\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        print('RESOURCE GOT EXHAUSTED')\n",
    "        with open(path2SaveError + \"ErrParams\"+ run_name + \".csv\",\"a\") as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            for key,value in params.items():\n",
    "                csv_writer.writerow([key,value])\n",
    "        csv_file.close()\n",
    "        session_num += 1\n",
    "        valid_acc = last_acc\n",
    "        tf.keras.backend.clear_session()    \n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'batch_size': (4,64),'learning_rate': (0, 4),'L2_2': (0, 1),'L2_3': (0, 1),\n",
    "           'BNConv1': (0, 1),'BNConv2': (0, 1),'BNDense1': (0, 1),'BNDense2': (0, 1),'BNDense3': (0, 1)\n",
    "         }\n",
    "CNN_BAYESIAN = BayesianOptimization(generate_cnn,pbounds=pbounds,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = JSONLogger(path=\"/home/airadiomicslab/OneDrive/logs/RTOGBayesianlogs.json\")\n",
    "CNN_BAYESIAN.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiM2M5MDk2YWItYzRiMS00MThlLTljZTctMDFmNDJkYWIwZmU5In0=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 1\n",
    "neptune.init('kundan25/RTOG0617',api_token=api_token)\n",
    "neptune.set_project('kundan25/RTOG0617')\n",
    "init_points = 25\n",
    "n_iter = 2000\n",
    "with neptune.create_experiment(name='BayesianRun3',params=pbounds) as BayesianRunOptimizer:\n",
    "    CNN_BAYESIAN.maximize(init_points = init_points, n_iter = n_iter, acq = 'ei', xi = 0.0)\n",
    "    BayesianRunOptimizer.append_tag('BayesianOptimizerRun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'batch_size': (8,18),'epochs':(25,1000),'filter2': (28,36),'filter3': (122,134),'f1_k1': (3, 7),\n",
    "           'f1_k2': (3, 7),'f1_k3': (3, 7),'learning_rate': (0.0001, 0.01),'dropout_rate1': (0.2, 0.4),\n",
    "           'L2_2': (0.0001, 1),'L2_3': (0.0001, 1),'D_droput2': (0.2, 0.4),'D_droput3': (0.2, 0.4),\n",
    "           'BNConv1': (0, 1),'BNConv2': (0, 1),'BNDense1': (0, 1),'BNDense2': (0, 1),'BNDense3': (0, 1)\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,value in enumerate(CNN_BAYESIAN.max['params']):\n",
    "        BayesianRunOptimizer.log_metric(value,CNN_BAYESIAN.max['Best_params'][value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Model accuracy is: %f' % CNN_BAYESIAN.max['target'])\n",
    "print('Best Model parameters are: %s' % CNN_BAYESIAN.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt.util import load_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpbounds = {'batch_size': (8,8),'epochs':(694,694),'filter2': (28,28),'filter3': (122,122),'f1_k1': (7, 7),\n",
    "           'f1_k2': (3, 3),'f1_k3': (3, 3),'learning_rate': (0.0001, 0.0001),'dropout_rate1': (0.2, 0.2),\n",
    "           'L2_2': (0.0001, 0.0001),'L2_3': (0.0001, 0.0001),'D_droput2': (0.2, 0.2),'D_droput3': (0.4, 0.4),\n",
    "           'BNConv1': (0, 0),'BNConv2': (0, 0),'BNDense1': (0, 0),'BNDense2': (1, 1),'BNDense3': (0, 0)\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_CNN_BAYESIAN = BayesianOptimization(generate_cnn,pbounds=pbounds,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 106\n",
    "neptune.init('kundan25/EsoCRPredictionBaysian',api_token=api_token)\n",
    "neptune.set_project('kundan25/EsoCRPredictionBaysian')\n",
    "init_points = 0\n",
    "n_iter = 400\n",
    "with neptune.create_experiment(name='BayesianRun',params=pbounds) as BayesianRunOptimizer:\n",
    "    CNN_BAYESIAN.maximize(init_points = init_points, n_iter = n_iter, acq = 'ei', xi = 0.0)\n",
    "    BayesianRunOptimizer.append_tag('BayesianOptimizerRun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(CNN_BAYESIAN.space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt.util import load_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_logs(CNN_BAYESIAN, logs=[\"/home/airadiomicslab/OneDrive/logs/Bayesianlogs2.json\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New optimizer is now aware of {} points.\".format(len(CNN_BAYESIAN.space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CNN_BAYESIAN.maximize(init_points = 0, n_iter = 100, acq = 'ei', xi = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CNN_BAYESIAN.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "neptune": {
   "notebookId": "b0fb41a2-e362-4acf-81e3-c8f305140e72"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
